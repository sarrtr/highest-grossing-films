{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frws353HMTW1"
      },
      "source": [
        "For Wikipedia page parsing requests and BeautifulSoup libraries are used. Os and re are auxiliary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o9ZPuhziJ0zO"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "\n",
        "# URL of the Wikipedia page\n",
        "url = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n",
        "\n",
        "# GET request to fetch the page content and ensure that request was successful\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()\n",
        "\n",
        "# Parse HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find the table containing the highest-grossing films\n",
        "table = soup.find('table', {'class': 'wikitable'})\n",
        "\n",
        "# a list to store the extracted film data\n",
        "films = []\n",
        "\n",
        "# index for the sequential count of films\n",
        "index = 1\n",
        "\n",
        "# Iterate through each row in the table except the header\n",
        "for row in table.find_all('tr')[1:]:\n",
        "\n",
        "    # Extract all columns with tag 'td' in the row\n",
        "    columns = row.find_all('td')\n",
        "\n",
        "    # Extract film release year\n",
        "    release_year = int(columns[3].text.strip())\n",
        "\n",
        "    # Extract film box office\n",
        "    # and handle cases with excess symbols such as 'F8$1,238,764,765' or 'T$2,257,844,554'\n",
        "    box_office = columns[2].text.strip()\n",
        "    dollar_index = box_office.find('$')\n",
        "    box_office = box_office[dollar_index:]\n",
        "\n",
        "    # Extract all columns with tag 'th' in the row\n",
        "    columns = row.find_all('th')\n",
        "\n",
        "    # Extract film title\n",
        "    # and handle cases with excess symbols such as 'Ne Zha 2 †'\n",
        "    title = columns[0].text.strip()\n",
        "    translation_table = str.maketrans('', '', '†')\n",
        "    title = title.translate(translation_table)\n",
        "\n",
        "    # Extract URL of specific film page\n",
        "    link = columns[0].find('a')\n",
        "    url = 'https://en.wikipedia.org' + link['href']\n",
        "\n",
        "    # GET request to fetch the page content and ensure that request was successful\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Parse page content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract table with info about film\n",
        "    table = soup.find('table', {'class': 'infobox vevent'})\n",
        "\n",
        "    # Create folder to save film covers\n",
        "    output_folder = \"covers\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Extract cover URL and get request\n",
        "    image = table.find('img')\n",
        "    if image:\n",
        "        image_url = 'https:' + image['src']\n",
        "    response = requests.get(image_url)\n",
        "\n",
        "    # If request was successful, save cover in .jpg format to specified folder\n",
        "    # named as 'cover{sequential number of the film}.jpg'\n",
        "    if response.status_code == 200:\n",
        "      filename = f\"cover{index}.jpg\"\n",
        "      save_path = os.path.join(output_folder, filename)\n",
        "      with open(save_path, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    index += 1\n",
        "\n",
        "    # Extract film country(-ies),\n",
        "    # handling different cases related to number of countries and html tags\n",
        "    country_th = table.find('th', string='Country')\n",
        "    if country_th:\n",
        "      country_td = country_th.find_next('td')\n",
        "      country = country_td.text.strip()\n",
        "    else:\n",
        "      country_th = table.find('th', string='Countries')\n",
        "      country_td = country_th.find_next('td')\n",
        "      if country_td.find_all('li'):\n",
        "        country = (', ').join([c.contents[0].strip() for c in country_td.find_all('li')])\n",
        "      else:\n",
        "        parts = [text.strip() for text in country_td.stripped_strings]\n",
        "        country = ', '.join(parts)\n",
        "\n",
        "    # Extract film director(s)\n",
        "    director_th = table.find('th', string='Directed by')\n",
        "    director_td = director_th.find_next('td')\n",
        "    director = set()\n",
        "\n",
        "    # Handle case when there are several directors\n",
        "    if director_td.find_all('li'):\n",
        "      li_tags = director_td.find_all('li')\n",
        "\n",
        "      for li in li_tags:\n",
        "        li_text = li.get_text()\n",
        "        if li_text:\n",
        "          dirs = li_text.split('\\n')\n",
        "          for dir in dirs:\n",
        "            if dir:\n",
        "              director.add(dir)\n",
        "\n",
        "      for li in li_tags:\n",
        "        a_tag = li.find('a')\n",
        "        if not a_tag:\n",
        "          director.add(li.get_text(strip=True))\n",
        "\n",
        "    # Case when there is one director\n",
        "    else:\n",
        "      director.add(director_td.get_text(separator=', '))\n",
        "\n",
        "    # Handle case when directors are duplicated\n",
        "    director_copy = director.copy()\n",
        "    for d1 in director_copy:\n",
        "      for d2 in director_copy:\n",
        "        if d1 != d2:\n",
        "          if d1 in d2 and d2 in director:\n",
        "            director.remove(d2)\n",
        "\n",
        "    # Convert set to string\n",
        "    director = ', '.join(director)\n",
        "    translation_table = str.maketrans('', '', '0123456789[]')\n",
        "    director = director.translate(translation_table)\n",
        "\n",
        "    # Add a dictionary with film data to the list with all films\n",
        "    films.append({\n",
        "            'title': title,\n",
        "            'release_year': release_year,\n",
        "            'director': director,\n",
        "            'box_office': box_office,\n",
        "            'country': country\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For data storage SQLite is used."
      ],
      "metadata": {
        "id": "O_LM7VobhAIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Connect to the SQLite database (or create it if it doesn't exist)\n",
        "conn = sqlite3.connect('films.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create the 'films' table\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS films (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    title TEXT NOT NULL,\n",
        "    release_year INTEGER,\n",
        "    director TEXT,\n",
        "    box_office TEXT,\n",
        "    country TEXT\n",
        ")\n",
        "''')\n",
        "\n",
        "# Commit the changes\n",
        "conn.commit()\n",
        "\n",
        "# Extract data from list films and add to the database\n",
        "for film in films:\n",
        "    cursor.execute('''\n",
        "    INSERT INTO films (title, release_year, director, box_office, country)\n",
        "    VALUES (?, ?, ?, ?, ?)\n",
        "    ''', (film['title'], film['release_year'], film['director'], film['box_office'], film['country']))\n",
        "\n",
        "# Commit the changes\n",
        "conn.commit()\n",
        "\n",
        "# Close the connection\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "L50dpww5g_0g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since GitHub Pages is static hosting, database content is exported to a JSON file."
      ],
      "metadata": {
        "id": "_MCsYYLSiJdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Connect to database\n",
        "conn = sqlite3.connect('films.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Fetch all films with their id and features\n",
        "cursor.execute('SELECT id, title, release_year, director, box_office, country FROM films')\n",
        "films = cursor.fetchall()\n",
        "\n",
        "# Convert to a list of dictionaries\n",
        "films_list = []\n",
        "for film in films:\n",
        "    films_list.append({'id': film[0],\n",
        "        'title': film[1],\n",
        "        'release_year': film[2],\n",
        "        'director': film[3],\n",
        "        'box_office': film[4],\n",
        "        'country': film[5]\n",
        "    })\n",
        "\n",
        "# Write to a JSON file\n",
        "with open('films.json', 'w') as f:\n",
        "    json.dump(films_list, f, indent=4)"
      ],
      "metadata": {
        "id": "-vagc_uViJ2z"
      },
      "execution_count": 4,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}